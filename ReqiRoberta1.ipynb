{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "11-JY8wmuQEcwd526lJhssLLON-RrZiq_",
      "authorship_tag": "ABX9TyMB1Ev3FvlEz1JNSl0/iCc7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanglapk/NFRDeepLearningFramework/blob/main/ReqiRoberta1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VRm9dTQykSL"
      },
      "outputs": [],
      "source": [
        "# We won't need TensorFlow here\n",
        "#!pip uninstall -y tensorflow\n",
        "# Install `transformers` from master\n",
        "#!pip install git+https://github.com/huggingface/transformers\n",
        "#!pip list | grep -E 'transformers|tokenizers'\n",
        "# transformers version at notebook update --- 2.11.0\n",
        "# tokenizers version at notebook update --- 0.8.0rc1\n",
        "!pip install transformers\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import os\n",
        "import time\n",
        "# Initialize a tokenizer\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "paths = [\"/content/Req2VecCorpusOSM.txt\"]  # Replace with the actual file path on your computer\n",
        "tokenizer.train(files=paths, vocab_size=52000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "    \"\",\n",
        "])\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training completed in {elapsed_time:.2f} seconds.\")\n",
        "tokenizer.save_model( \"/content/drive/MyDrive/SoftwareRequirement/tokenizer_output\",\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"/content/drive/MyDrive/SoftwareRequirement/tokenizer_output/en-vocab.json\",\n",
        "    \"/content/drive/MyDrive/SoftwareRequirement/tokenizer_output/en-merges.txt\",\n",
        ")\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"\", tokenizer.token_to_id(\"\")),\n",
        "    (\"\", tokenizer.token_to_id(\"\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=514)\n",
        "print(tokenizer.encode(\"The message must be updated for all users in a chat within 0.1 seconds, given that all users are online and have LTE connection or better.\"))\n",
        "tokenizer.encode(\"The message must be updated for all users in a chat within 0.1 seconds, given that all users are online and have LTE connection or better.\").tokens"
      ],
      "metadata": {
        "id": "upu6tx5Iy4QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] accelerate==0.20.1\n",
        "import torch\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
        "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define your model configuration\n",
        "config = RobertaConfig(\n",
        "    vocab_size=50265,\n",
        "    hidden_size=768,\n",
        "    type_vocab_size=1,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=12,\n",
        "    intermediate_size=4096,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    layer_norm_eps=1e-5,\n",
        "    use_cache=True,\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        "\n",
        ")\n",
        "\n",
        "# Initialize your model\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = RobertaTokenizer(\n",
        "    vocab_file=\"/content/drive/MyDrive/SoftwareRequirement/tokenizer_output/en-vocab.json\",# Output of the trained tokenizer\n",
        "    merges_file = \"/content/drive/MyDrive/SoftwareRequirement/tokenizer_output/en-merges.txt\",#Output of the trained tokenizer\n",
        "    max_len=514,\n",
        ")\n",
        "\n",
        "# Define your training data\n",
        "train_file = \"/content/Req2VecCorpusOSM.txt\"\n",
        "\n",
        "# Create a dataset from your text data\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=train_file,\n",
        "    block_size=514,\n",
        ")\n",
        "# Define the path to your evaluation text file\n",
        "eval_file = \"/content/Req2VecCorpus.txt\"\n",
        "\n",
        "# Create an evaluation dataset from your text data\n",
        "eval_dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=eval_file,\n",
        "    block_size=514,\n",
        ")\n",
        "\n",
        "# Define a data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,  # Set mlm to True for masked language modeling\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/SoftwareRequirement/training_output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=5_000,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,  # Number of warm-up steps for the learning rate scheduler\n",
        "    weight_decay=0.01,  # L2 regularization term\n",
        "    logging_dir=\"/content/drive/MyDrive/SoftwareRequirement/training_logs\",  # Logging directory\n",
        "    logging_steps=100,  # How often training logs are written\n",
        "    gradient_accumulation_steps=1,  # Accumulate gradients over multiple steps\n",
        "    seed=42,  # Random seed for reproducibility\n",
        "    label_smoothing_factor=0.1,  # Label smoothing factor\n",
        "    report_to=\"tensorboard\",  # Report results to TensorBoard\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/SoftwareRequirement/models/version1/ReqiRoberta\")\n"
      ],
      "metadata": {
        "id": "ZURGO9HI3q_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}